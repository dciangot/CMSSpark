{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "\n",
    "#sqlContext = SQLContext(sc)\n",
    "#data_spark = sqlContext.read.parquet(\"hdfs://analytix/cms/users/dciangot/ws_condor_201801\")\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data_spark.toPandas()\n",
    "months_17 = [\n",
    "         '17_01',\n",
    "         '17_02',\n",
    "         '17_03',\n",
    "         '17_04',\n",
    "         '17_05',\n",
    "         '17_06',\n",
    "         '17_07',\n",
    "         '17_08',\n",
    "         '17_09',\n",
    "         '17_10',\n",
    "         '17_11',\n",
    "         '17_12'\n",
    "        ]\n",
    "\n",
    "months_18 = [\n",
    "         '201801',\n",
    "         '18_02',\n",
    "         '18_03',\n",
    "         '18_04',\n",
    "         '18_05',\n",
    "         '18_06',\n",
    "         '18_07',\n",
    "         '18_08',\n",
    "         '18_09',\n",
    "         '18_10',\n",
    "         '18_11',\n",
    "         '18_12',\n",
    "        ]\n",
    "\n",
    "months_19 = [\n",
    "         '19_01',\n",
    "         '19_02',\n",
    "         '19_03',\n",
    "         '19_04',\n",
    "        ]\n",
    "\n",
    "data_tmp = None\n",
    "\n",
    "for month in months_19:\n",
    "    data_month = pd.read_pickle('ws_%s.pkl' % month)\n",
    "\n",
    "    data = pd.concat([data_tmp, data_month], sort=False)\n",
    "    \n",
    "    data_tmp = data\n",
    "\n",
    "data['day'] = pd.to_datetime(data['day'],unit='s')\n",
    "\n",
    "data.fillna(-1)\n",
    "\n",
    "start_date = datetime.datetime(2018,12,31)\n",
    "\n",
    "regions = ['T2_FR_', 'T2_IT_', 'T2_US_', 'T2_DE_', 'T2_ES_']\n",
    "for region in regions:\n",
    "    data_tmp = data[(data['CMSSite'].str.startswith(region)) & (data['day'] > start_date)]\n",
    "    data_tmp.to_pickle('ws_%s19.pkl.gz' % region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = data[(data['day'] > start_date)]\n",
    "data_tmp.to_pickle('ws_all_19.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "sparkconnect": {
   "bundled_options": [
    "CMSSpark"
   ],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
